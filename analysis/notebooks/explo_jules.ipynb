{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de phrases\n",
    "source_sentences = [\"I eat bread\", \"I like coffee\"]\n",
    "target_sentences = [\"Je mange du pain de mie\", \"Je aime le café\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulateur de tokenizer (ex : Hugging Face)\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.start_token = \"<start>\"\n",
    "        self.end_token = \"<end>\"\n",
    "        self.pad_id = vocab[self.pad_token]\n",
    "        self.start_id = vocab[self.start_token]\n",
    "        self.end_id = vocab[self.end_token]\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        # Sépare les mots et la ponctuation\n",
    "        return re.findall(r\"\\w+|[^\\w\\s]\", sentence.lower())\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        tokens = [self.start_token] + self.tokenize(sentence) + [self.end_token]\n",
    "        return [self.vocab[token] for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire fictif (vocabulaire)\n",
    "vocab = {\n",
    "    \"<pad>\": 0,\n",
    "    \"<start>\": 1,\n",
    "    \"<end>\": 2,\n",
    "    \"i\": 3,\n",
    "    \"eat\": 4,\n",
    "    \"bread\": 5,\n",
    "    \".\": 6,\n",
    "    \"je\": 7,\n",
    "    \"mange\": 8,\n",
    "    \"du\": 9,\n",
    "    \"pain\": 10,\n",
    "    \"like\": 11,\n",
    "    \"coffee\": 12,\n",
    "    \"aime\": 13,\n",
    "    \"le\": 14,\n",
    "    \"café\": 15,\n",
    "    \"de\": 16,\n",
    "    \"mie\": 17,\n",
    "}\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage des phrases\n",
    "source_encoded = [\n",
    "    torch.tensor(tokenizer.encode(sentence)) for sentence in source_sentences\n",
    "]\n",
    "target_encoded = [\n",
    "    torch.tensor(tokenizer.encode(sentence)) for sentence in target_sentences\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 3, 4, 5, 2]), tensor([ 1,  3, 11, 12,  2])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 1,  7,  8,  9, 10, 16, 17,  2]), tensor([ 1,  7, 13, 14, 15,  2])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding des séquences\n",
    "source_padded = pad_sequence(\n",
    "    source_encoded, batch_first=True, padding_value=tokenizer.pad_id\n",
    ")\n",
    "target_padded = pad_sequence(\n",
    "    target_encoded, batch_first=True, padding_value=tokenizer.pad_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  3,  4,  5,  2],\n",
       "        [ 1,  3, 11, 12,  2]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  7,  8,  9, 10, 16, 17,  2],\n",
       "        [ 1,  7, 13, 14, 15,  2,  0,  0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Padded (Inputs pour l'encodeur) :\n",
      " tensor([[ 1,  3,  4,  5,  2],\n",
      "        [ 1,  3, 11, 12,  2]])\n",
      "Decoder Input (Inputs pour le décodeur) :\n",
      " tensor([[ 1,  7,  8,  9, 10, 16, 17],\n",
      "        [ 1,  7, 13, 14, 15,  2,  0]])\n",
      "Decoder Target (Cibles pour l'entraînement) :\n",
      " tensor([[ 7,  8,  9, 10, 16, 17,  2],\n",
      "        [ 7, 13, 14, 15,  2,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "# Décalage des targets pour Teacher Forcing\n",
    "# Input du décodeur (décalé) : enlever le dernier token\n",
    "decoder_input = target_padded[:, :-1]\n",
    "# Target attendue : enlever le premier token\n",
    "decoder_target = target_padded[:, 1:]\n",
    "\n",
    "# Affichage\n",
    "print(\"Source Padded (Inputs pour l'encodeur) :\\n\", source_padded)\n",
    "print(\"Decoder Input (Inputs pour le décodeur) :\\n\", decoder_input)\n",
    "print(\"Decoder Target (Cibles pour l'entraînement) :\\n\", decoder_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jules\\anaconda3\\envs\\py310-transformer\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape (Batch x SeqLen x VocabSize): torch.Size([2, 5, 20])\n",
      "Probabilités pour le premier token: tensor([0.0361, 0.0194, 0.0603, 0.0614, 0.0728, 0.1461, 0.0292, 0.0710, 0.0499,\n",
      "        0.0499, 0.0241, 0.0223, 0.0328, 0.0715, 0.0459, 0.0172, 0.0665, 0.0728,\n",
      "        0.0255, 0.0254], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jules\\anaconda3\\envs\\py310-transformer\\lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Paramètres globaux\n",
    "SOURCE_VOCAB_SIZE = 20  # Taille fictive du vocabulaire anglais\n",
    "TARGET_VOCAB_SIZE = 20  # Taille fictive du vocabulaire français\n",
    "EMBEDDING_DIM = 512  # Dimension des embeddings\n",
    "NUM_HEADS = 8  # Nombre de têtes dans l'attention multi-tête\n",
    "NUM_ENCODER_LAYERS = 6  # Nombre de couches dans l'encodeur\n",
    "NUM_DECODER_LAYERS = 6  # Nombre de couches dans le décodeur\n",
    "FFN_HIDDEN_DIM = 2048  # Taille des couches intermédiaires\n",
    "MAX_SEQ_LEN = 100  # Longueur maximale des séquences\n",
    "PAD_IDX = 0  # Index du token de padding\n",
    "\n",
    "\n",
    "# Modèle Transformer complet\n",
    "class TranslationTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_vocab_size,\n",
    "        target_vocab_size,\n",
    "        embedding_dim,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        ffn_hidden_dim,\n",
    "        pad_idx,\n",
    "        max_seq_len=100,\n",
    "    ):\n",
    "        super(TranslationTransformer, self).__init__()\n",
    "        self.embedding_src = nn.Embedding(\n",
    "            source_vocab_size, embedding_dim, padding_idx=pad_idx\n",
    "        )\n",
    "        self.embedding_tgt = nn.Embedding(\n",
    "            target_vocab_size, embedding_dim, padding_idx=pad_idx\n",
    "        )\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim, max_seq_len)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=ffn_hidden_dim,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embedding_dim, target_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask):\n",
    "        # Embedding + Position Encoding\n",
    "        src_emb = self.positional_encoding(self.embedding_src(src))\n",
    "        tgt_emb = self.positional_encoding(self.embedding_tgt(tgt))\n",
    "\n",
    "        # Transformer Forward Pass\n",
    "        transformer_out = self.transformer(\n",
    "            src_emb.permute(1, 0, 2),  # SeqLen x Batch x Embedding\n",
    "            tgt_emb.permute(1, 0, 2),  # SeqLen x Batch x Embedding\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "        )\n",
    "\n",
    "        # Output projection (vocab probabilities)\n",
    "        output = self.fc_out(\n",
    "            transformer_out.permute(1, 0, 2)\n",
    "        )  # Batch x SeqLen x VocabSize\n",
    "        return output\n",
    "\n",
    "\n",
    "# Encodage Positionnel\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_seq_len, embedding_dim)\n",
    "        self.encoding.requires_grad = False  # Pas d'apprentissage\n",
    "\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embedding_dim, 2).float()\n",
    "            * (-torch.log(torch.tensor(10000.0)) / embedding_dim)\n",
    "        )\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.encoding[:seq_len, :].to(x.device)\n",
    "\n",
    "\n",
    "# Masque pour l'encodeur et le décodeur\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    return torch.triu(torch.ones(sz, sz) * float(\"-inf\"), diagonal=1)\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialisation du modèle\n",
    "    model = TranslationTransformer(\n",
    "        source_vocab_size=SOURCE_VOCAB_SIZE,\n",
    "        target_vocab_size=TARGET_VOCAB_SIZE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "        ffn_hidden_dim=FFN_HIDDEN_DIM,\n",
    "        pad_idx=PAD_IDX,\n",
    "        max_seq_len=MAX_SEQ_LEN,\n",
    "    )\n",
    "\n",
    "    # Exemple de données\n",
    "    src = torch.tensor(\n",
    "        [[1, 2, 3, 4, 5], [6, 7, 8, 0, 0]]\n",
    "    )  # Batch x SeqLen (indices du vocabulaire source)\n",
    "    tgt = torch.tensor(\n",
    "        [[1, 7, 8, 9, 0], [6, 14, 15, 0, 0]]\n",
    "    )  # Batch x SeqLen (indices du vocabulaire cible)\n",
    "\n",
    "    # Masques\n",
    "    src_mask = None  # Pas nécessaire pour l'encodeur avec des séquences complètes\n",
    "    tgt_mask = generate_square_subsequent_mask(\n",
    "        tgt.size(1)\n",
    "    )  # Masque triangulaire pour le décodeur\n",
    "    src_padding_mask = src == PAD_IDX  # Masque pour ignorer les tokens de padding\n",
    "    tgt_padding_mask = tgt == PAD_IDX  # Masque pour ignorer les tokens de padding\n",
    "\n",
    "    # Passage dans le modèle\n",
    "    out = model(src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask)\n",
    "    print(\"Output shape (Batch x SeqLen x VocabSize):\", out.shape)\n",
    "\n",
    "    # Exemple de probabilité softmax pour le 1er batch et 1er token\n",
    "    print(\"Probabilités pour le premier token:\", F.softmax(out[0, 0], dim=-1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
